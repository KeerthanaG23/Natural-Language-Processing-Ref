{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd25fbf",
   "metadata": {},
   "source": [
    "## Building an NLP Sentiment Analysis Pipeline In Python\n",
    "Reference:https://www.linkedin.com/advice/0/how-do-you-design-implement-nlp-pipelines#:~:text=An%20NLP%20pipeline%20is%20a,entity%20recognition%2C%20or%20sentiment%20analysis.\n",
    "\n",
    "2.https://www.geeksforgeeks.org/natural-language-processing-nlp-pipeline/\n",
    "####  1.Data Acquisition\n",
    "####   2.Text Cleaning\n",
    "Unicode normalisation:Symbols,Emojis,Special Characters\n",
    "Regex:String pattern based removal of email,Phine number,URL\n",
    "Spellingm Correction:Web scraped data - Create a corpus or dictionary of misspelled word\n",
    "####   3.Text Preprocessing\n",
    "Words to be separated at the minimum level\n",
    "Tokenization\n",
    "Lowercasing\n",
    "Stop words removal\n",
    "Stemming/Lemmatization\n",
    "POS tagging - Assign Parts of speech to each word in the text(NER,Sentimental Analysis& Machine translation)\n",
    "####   4.Feature Engineering\n",
    "Text vectorization/Representation\n",
    "Classical approach:\n",
    "    One hot encoding\n",
    "    Bag of words\n",
    "    Bag of n-grams\n",
    "    TF-TDF\n",
    "Neural approach or Word Embedding:\n",
    "    To understand the contextual meaning\n",
    "        Continous Bag of word\n",
    "        Skip gram \n",
    "    Pre trained word embedding - Use large corpus --Import Gensim or hugging face Word2Vec by Google,GloVe by stanford    \n",
    "####   5.Building Model\n",
    "\n",
    "####   6.Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae78bc",
   "metadata": {},
   "source": [
    "### Data Acquisition - https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85a9363",
   "metadata": {},
   "source": [
    "### Import necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9f4131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ad1a1",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf2c83a8",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "860740c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reference  = pd.read_csv(r\"E:\\NLP\\Lab\\dataset\\train.csv\",encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c751d023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment Time of Tweet Age of User  \\\n",
       "0  I`d have responded, if I were going   neutral       morning        0-20   \n",
       "1                             Sooo SAD  negative          noon       21-30   \n",
       "2                          bullying me  negative         night       31-45   \n",
       "3                       leave me alone  negative       morning       46-60   \n",
       "4                        Sons of ****,  negative          noon       60-70   \n",
       "\n",
       "       Country  Population -2020  Land Area (Km²)  Density (P/Km²)  \n",
       "0  Afghanistan          38928346         652860.0               60  \n",
       "1      Albania           2877797          27400.0              105  \n",
       "2      Algeria          43851044        2381740.0               18  \n",
       "3      Andorra             77265            470.0              164  \n",
       "4       Angola          32866272        1246700.0               26  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reference.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5deb3370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file):\n",
    "        data = pd.read_csv(file,encoding='latin1')\n",
    "        data.drop_duplicates(inplace=True)\n",
    "        data.dropna(inplace=True)\n",
    "        selected_colums=['text','sentiment']\n",
    "        data=data[selected_colums]\n",
    "        data=pd.DataFrame(data)\n",
    "        return data\n",
    "    \n",
    "train_data = load_dataset(r\"E:\\NLP\\Lab\\dataset\\train.csv\")\n",
    "test_data =load_dataset(r\"E:\\NLP\\Lab\\dataset\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "400feea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 27480 entries, 0 to 27480\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       27480 non-null  object\n",
      " 1   sentiment  27480 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 644.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787d9ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d91d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27480 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0                    I`d have responded, if I were going   neutral\n",
       "1          Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "2                              my boss is bullying me...  negative\n",
       "3                         what interview! leave me alone  negative\n",
       "4       Sons of ****, why couldn`t they put them on t...  negative\n",
       "...                                                  ...       ...\n",
       "27476   wish we could come see u on Denver  husband l...  negative\n",
       "27477   I`ve wondered about rake to.  The client has ...  negative\n",
       "27478   Yay good for both of you. Enjoy the break - y...  positive\n",
       "27479                         But it was worth it  ****.  positive\n",
       "27480     All this flirting going on - The ATG smiles...   neutral\n",
       "\n",
       "[27480 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "494f4281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3534 entries, 0 to 3533\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       3534 non-null   object\n",
      " 1   sentiment  3534 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 82.8+ KB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "254eff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reference = data_reference.iloc[:,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6280de9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                    I`d have responded, if I were going   neutral  \n",
       "1                                               Sooo SAD  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                          Sons of ****,  negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                      , don`t force  negative  \n",
       "27478                          Yay good for both of you.  positive  \n",
       "27479                         But it was worth it  ****.  positive  \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "828f5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reference = data_reference.drop(['textID','selected_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "982f4219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0                    I`d have responded, if I were going   neutral\n",
       "1          Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "2                              my boss is bullying me...  negative\n",
       "3                         what interview! leave me alone  negative\n",
       "4       Sons of ****, why couldn`t they put them on t...  negative\n",
       "...                                                  ...       ...\n",
       "27476   wish we could come see u on Denver  husband l...  negative\n",
       "27477   I`ve wondered about rake to.  The client has ...  negative\n",
       "27478   Yay good for both of you. Enjoy the break - y...  positive\n",
       "27479                         But it was worth it  ****.  positive\n",
       "27480     All this flirting going on - The ATG smiles...   neutral\n",
       "\n",
       "[27481 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe0dc2",
   "metadata": {},
   "source": [
    "##### Label Encoding for the o/p columns - +ve , -ve and zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1afbd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_data['sentiment'])\n",
    "y_test = le.transform(test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59f33bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e35c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, ..., 0, 2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc39d6",
   "metadata": {},
   "source": [
    "### Text Cleaning "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f63861e",
   "metadata": {},
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "punctuations=string.punctuation\n",
    "punctuations\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14a30f65",
   "metadata": {},
   "source": [
    "b=\" Hello     all    \"\n",
    "b=b.strip()\n",
    "print([b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fa66a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(data:str):\n",
    "    data = data.strip()#Remove leading white spaces\n",
    "    data = data.lower()#Convert to lower case\n",
    "    url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    data = re.sub(url_pattern, \"\", data)\n",
    "    username_pattern = re.compile(r\"@\\w+\")\n",
    "    data = re.sub(username_pattern, \"\", data)\n",
    "    hashtag_pattern = re.compile(r\"#\\w+\")\n",
    "    data = re.sub(hashtag_pattern, \"\", data)\n",
    "    data = re.sub(r\"([a-zA-Z])\\1{2,}\", r'\\1', data)\n",
    "    data = re.sub(r'[^a-zA-Z\\s]',\"\",data)#Remove special characters\n",
    "    return data\n",
    "\n",
    "train_data['preprocess_1']=train_data['text'].apply(preprocessing_1)\n",
    "test_data['preprocess_1']=test_data['text'].apply(preprocessing_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03e88051",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['preprocess_1']=train_data['text'].apply(preprocessing_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83b4759b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocess_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>so sad i will miss you here in san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sons of  why couldnt they put them on the rele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>negative</td>\n",
       "      <td>wish we could come see u on denver  husband lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>ive wondered about rake to  the client has mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>positive</td>\n",
       "      <td>yay good for both of you enjoy the break  you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "      <td>but it was worth it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>all this flirting going on  the atg smiles yay...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27480 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  \\\n",
       "0                    I`d have responded, if I were going   neutral   \n",
       "1          Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "2                              my boss is bullying me...  negative   \n",
       "3                         what interview! leave me alone  negative   \n",
       "4       Sons of ****, why couldn`t they put them on t...  negative   \n",
       "...                                                  ...       ...   \n",
       "27476   wish we could come see u on Denver  husband l...  negative   \n",
       "27477   I`ve wondered about rake to.  The client has ...  negative   \n",
       "27478   Yay good for both of you. Enjoy the break - y...  positive   \n",
       "27479                         But it was worth it  ****.  positive   \n",
       "27480     All this flirting going on - The ATG smiles...   neutral   \n",
       "\n",
       "                                            preprocess_1  \n",
       "0                      id have responded if i were going  \n",
       "1               so sad i will miss you here in san diego  \n",
       "2                                 my boss is bullying me  \n",
       "3                          what interview leave me alone  \n",
       "4      sons of  why couldnt they put them on the rele...  \n",
       "...                                                  ...  \n",
       "27476  wish we could come see u on denver  husband lo...  \n",
       "27477  ive wondered about rake to  the client has mad...  \n",
       "27478  yay good for both of you enjoy the break  you ...  \n",
       "27479                              but it was worth it    \n",
       "27480  all this flirting going on  the atg smiles yay...  \n",
       "\n",
       "[27480 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d0edef9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocess_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>last session of the day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>shanghai is also really exciting precisely  sk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>recession hit veronique branquinho she has to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy bday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>i like it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>its at  am im very tired but i cant sleep  but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>All alone in this old house again.  Thanks for...</td>\n",
       "      <td>positive</td>\n",
       "      <td>all alone in this old house again  thanks for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>I know what you mean. My little dog is sinkin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i know what you mean my little dog is sinking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>_sutra what is your next youtube video gonna b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>sutra what is your next youtube video gonna be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>omgssh  ang cute ng bby</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3534 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment  \\\n",
       "0     Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1      Shanghai is also really exciting (precisely -...  positive   \n",
       "2     Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3                                           happy bday!  positive   \n",
       "4                http://twitpic.com/4w75p - I like it!!  positive   \n",
       "...                                                 ...       ...   \n",
       "3529  its at 3 am, im very tired but i can`t sleep  ...  negative   \n",
       "3530  All alone in this old house again.  Thanks for...  positive   \n",
       "3531   I know what you mean. My little dog is sinkin...  negative   \n",
       "3532  _sutra what is your next youtube video gonna b...  positive   \n",
       "3533   http://twitpic.com/4woj2 - omgssh  ang cute n...  positive   \n",
       "\n",
       "                                           preprocess_1  \n",
       "0                             last session of the day    \n",
       "1     shanghai is also really exciting precisely  sk...  \n",
       "2     recession hit veronique branquinho she has to ...  \n",
       "3                                            happy bday  \n",
       "4                                             i like it  \n",
       "...                                                 ...  \n",
       "3529  its at  am im very tired but i cant sleep  but...  \n",
       "3530  all alone in this old house again  thanks for ...  \n",
       "3531  i know what you mean my little dog is sinking ...  \n",
       "3532  sutra what is your next youtube video gonna be...  \n",
       "3533                            omgssh  ang cute ng bby  \n",
       "\n",
       "[3534 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15fb279f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Keerthana\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e0954a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m [lemma\u001b[38;5;241m.\u001b[39mlemmatize(word,pos\u001b[38;5;241m=\u001b[39mget_pos(word))\u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m---> 11\u001b[0m train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocess_2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtrain_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocess_1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocessing_2)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlpenv\\Lib\\site-packages\\pandas\\core\\series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4791\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4909\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4910\u001b[0m         func,\n\u001b[0;32m   4911\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4912\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4913\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4914\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4915\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlpenv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlpenv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlpenv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlpenv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36mpreprocessing_2\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tag_dict\u001b[38;5;241m.\u001b[39mget(tag,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m lemma \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39mWordNetLemmatizer()\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m [lemma\u001b[38;5;241m.\u001b[39mlemmatize(word,pos\u001b[38;5;241m=\u001b[39mget_pos(word))\u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m, in \u001b[0;36mpreprocessing_2.<locals>.get_pos\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_pos\u001b[39m(word):\n\u001b[1;32m----> 4\u001b[0m     tag \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mpos_tag([word])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m      5\u001b[0m     tag_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tag_dict\u001b[38;5;241m.\u001b[39mget(tag,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlpenv\\Lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlpenv\\Lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlpenv\\Lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         find(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtaggers/averaged_perceptron_tagger/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m PICKLE)\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\nlpenv\\Lib\\site-packages\\nltk\\data.py:522\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Check each item in our path\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_ \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;66;03m# Is the path item a zipfile?\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path_ \u001b[38;5;129;01mand\u001b[39;00m (os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path_) \u001b[38;5;129;01mand\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    523\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    524\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ZipFilePathPointer(path_, resource_name)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def preprocessing_2(data:str):\n",
    "    data = nltk.word_tokenize(data)\n",
    "    def get_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1].upper()\n",
    "        tag_dict = {\"N\":\"n\",\"V\":\"v\",\"R\":\"r\",\"J\":\"a\"}\n",
    "        return tag_dict.get(tag,\"n\")\n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    data = [lemma.lemmatize(word,pos=get_pos(word))for word in data]\n",
    "    return data\n",
    "    \n",
    "train_data['preprocess_2']=train_data[\"preprocess_1\"].apply(preprocessing_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d267aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['preprocess_2']=test_data[\"preprocess_1\"].apply(preprocessing_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be680d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863dc49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"documents\"] = train_data[\"preprocess_2\"].apply(lambda x : \" \".join(x))\n",
    "test_data[\"documents\"] = test_data[\"preprocess_2\"].apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42a2ec6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocess_1</th>\n",
       "      <th>preprocess_2</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>[id, have, responded, if, i, were, going]</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>so sad i will miss you here in san diego</td>\n",
       "      <td>[so, sad, i, will, miss, you, here, in, san, d...</td>\n",
       "      <td>so sad i will miss you here in san diego</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>[my, bos, is, bullying, me]</td>\n",
       "      <td>my bos is bullying me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>[what, interview, leave, me, alone]</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sons of  why couldnt they put them on the rele...</td>\n",
       "      <td>[son, of, why, couldnt, they, put, them, on, t...</td>\n",
       "      <td>son of why couldnt they put them on the releas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  \\\n",
       "0                I`d have responded, if I were going   neutral   \n",
       "1      Sooo SAD I will miss you here in San Diego!!!  negative   \n",
       "2                          my boss is bullying me...  negative   \n",
       "3                     what interview! leave me alone  negative   \n",
       "4   Sons of ****, why couldn`t they put them on t...  negative   \n",
       "\n",
       "                                        preprocess_1  \\\n",
       "0                  id have responded if i were going   \n",
       "1           so sad i will miss you here in san diego   \n",
       "2                             my boss is bullying me   \n",
       "3                      what interview leave me alone   \n",
       "4  sons of  why couldnt they put them on the rele...   \n",
       "\n",
       "                                        preprocess_2  \\\n",
       "0          [id, have, responded, if, i, were, going]   \n",
       "1  [so, sad, i, will, miss, you, here, in, san, d...   \n",
       "2                        [my, bos, is, bullying, me]   \n",
       "3                [what, interview, leave, me, alone]   \n",
       "4  [son, of, why, couldnt, they, put, them, on, t...   \n",
       "\n",
       "                                           documents  \n",
       "0                  id have responded if i were going  \n",
       "1           so sad i will miss you here in san diego  \n",
       "2                              my bos is bullying me  \n",
       "3                      what interview leave me alone  \n",
       "4  son of why couldnt they put them on the releas...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f1a6513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocess_1</th>\n",
       "      <th>preprocess_2</th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>last session of the day</td>\n",
       "      <td>[last, session, of, the, day]</td>\n",
       "      <td>last session of the day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>shanghai is also really exciting precisely  sk...</td>\n",
       "      <td>[shanghai, is, also, really, exciting, precise...</td>\n",
       "      <td>shanghai is also really exciting precisely sky...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>recession hit veronique branquinho she has to ...</td>\n",
       "      <td>[recession, hit, veronique, branquinho, she, h...</td>\n",
       "      <td>recession hit veronique branquinho she ha to q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy bday</td>\n",
       "      <td>[happy, bday]</td>\n",
       "      <td>happy bday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>i like it</td>\n",
       "      <td>[i, like, it]</td>\n",
       "      <td>i like it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  \\\n",
       "0  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1   Shanghai is also really exciting (precisely -...  positive   \n",
       "2  Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3                                        happy bday!  positive   \n",
       "4             http://twitpic.com/4w75p - I like it!!  positive   \n",
       "\n",
       "                                        preprocess_1  \\\n",
       "0                          last session of the day     \n",
       "1  shanghai is also really exciting precisely  sk...   \n",
       "2  recession hit veronique branquinho she has to ...   \n",
       "3                                         happy bday   \n",
       "4                                          i like it   \n",
       "\n",
       "                                        preprocess_2  \\\n",
       "0                      [last, session, of, the, day]   \n",
       "1  [shanghai, is, also, really, exciting, precise...   \n",
       "2  [recession, hit, veronique, branquinho, she, h...   \n",
       "3                                      [happy, bday]   \n",
       "4                                      [i, like, it]   \n",
       "\n",
       "                                           documents  \n",
       "0                            last session of the day  \n",
       "1  shanghai is also really exciting precisely sky...  \n",
       "2  recession hit veronique branquinho she ha to q...  \n",
       "3                                         happy bday  \n",
       "4                                          i like it  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "161d67f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_1 = preprocessing_1(\" Hellooooo I'ammmm keerthan@gmail.com #NLP is niceeeee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "749a223c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello iam keerthancom  is nice'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25152b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'iam', 'keerthancom', 'is', 'nice']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_2(res_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d102f8-7435-424c-a627-8f8bfd67691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= \" \".join(['hello', 'iam', 'keerthancom', 'is', 'nice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89654452-0d81-4c8f-9064-a350cb006662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello iam keerthancom is nice'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea02ab",
   "metadata": {},
   "source": [
    "### Creating a vocabulary from the unique words in the text - set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9d99ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 23462\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for words in train_data['preprocess_2']:\n",
    "    for word in words:\n",
    "        vocab.add(word)\n",
    "print(\"Vocabulary Size:\",len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f682631",
   "metadata": {},
   "source": [
    "#### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2092f9ff",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0060f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow = CountVectorizer()\n",
    "train_bow = bow.fit_transform(train_data['documents'])\n",
    "test_bow = bow.transform(test_data['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "876d9b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20028ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.6983588002263724\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.64      0.67      1001\n",
      "           1       0.64      0.73      0.68      1430\n",
      "           2       0.79      0.71      0.75      1103\n",
      "\n",
      "    accuracy                           0.70      3534\n",
      "   macro avg       0.71      0.69      0.70      3534\n",
      "weighted avg       0.71      0.70      0.70      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "model.fit(train_bow, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "predict = model.predict(test_bow)\n",
    "print(\"Accuracy Score :\", accuracy_score(y_test, predict), end='\\n\\n')\n",
    "print(classification_report(y_true = y_test, y_pred = predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86020370",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "acb41a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "td_idf = TfidfVectorizer()\n",
    "\n",
    "\n",
    "train_idf = td_idf.fit_transform(train_data['documents']) \n",
    "test_idf = td_idf.transform(test_data['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21394404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3534, 23436)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "906c235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.7085455574419921\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.64      0.68      1001\n",
      "           1       0.64      0.76      0.69      1430\n",
      "           2       0.81      0.71      0.76      1103\n",
      "\n",
      "    accuracy                           0.71      3534\n",
      "   macro avg       0.73      0.70      0.71      3534\n",
      "weighted avg       0.72      0.71      0.71      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "model.fit(train_idf, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "predict = model.predict(test_idf)\n",
    "print(\"Accuracy Score :\", accuracy_score(y_test, predict), end='\\n\\n')\n",
    "print(classification_report(y_true = y_test, y_pred = predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2872bd1f",
   "metadata": {},
   "source": [
    "### Continous Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8330d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "g_model = Word2Vec(sentences = train_data['preprocess_2'],vector_size=200,window=5, workers=5, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "411ad9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_vocab(word_l):\n",
    "    for word in word_l:\n",
    "        if word not in g_model.wv:\n",
    "            return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "train_vec = [g_model.wv[x].sum(axis = 0) if len(x) and in_vocab(x) else np.zeros((200)) for x in train_data['preprocess_2']]\n",
    "test_vec  = [g_model.wv[x].sum(axis = 0) if len(x) and in_vocab(x) else np.zeros((200)) for x in test_data['preprocess_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e37a163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.5070741369552915\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.26      0.37      1001\n",
      "           1       0.45      0.85      0.59      1430\n",
      "           2       0.72      0.28      0.41      1103\n",
      "\n",
      "    accuracy                           0.51      3534\n",
      "   macro avg       0.60      0.47      0.46      3534\n",
      "weighted avg       0.59      0.51      0.47      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "model.fit(train_vec, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "predict = model.predict(test_vec)\n",
    "print(\"Accuracy Score :\", accuracy_score(y_test, predict), end='\\n\\n')\n",
    "print(classification_report(y_true = y_test, y_pred = predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50fd85",
   "metadata": {},
   "source": [
    "### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11e4b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "g_model = Word2Vec(sentences=train_data['preprocess_2'], vector_size=200, window=5, workers=5, sg=1, epochs=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4cdba34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_vocab(word_l):\n",
    "    for word in word_l:\n",
    "        if word not in g_model.wv:\n",
    "            return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "train_vec = [g_model.wv[x].sum(axis = 0) if len(x) and in_vocab(x) else np.zeros((200)) for x in train_data[\"preprocess_2\"]]\n",
    "test_vec  = [g_model.wv[x].sum(axis = 0) if len(x) and in_vocab(x) else np.zeros((200)) for x in test_data[\"preprocess_2\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7244235c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.5079230333899264\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.26      0.37      1001\n",
      "           1       0.46      0.86      0.59      1430\n",
      "           2       0.70      0.28      0.40      1103\n",
      "\n",
      "    accuracy                           0.51      3534\n",
      "   macro avg       0.60      0.47      0.46      3534\n",
      "weighted avg       0.59      0.51      0.47      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "model.fit(train_vec, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "predict = model.predict(test_vec)\n",
    "print(\"Accuracy Score :\", accuracy_score(y_test, predict), end='\\n\\n')\n",
    "print(classification_report(y_true = y_test, y_pred = predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b040cf0",
   "metadata": {},
   "source": [
    "### WORD2VEC using GloVe of twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09657d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 758.5/758.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"glove-twitter-200\")\n",
    "\n",
    "shape_n = 200\n",
    "\n",
    "def in_vocab(word_l):\n",
    "    for word in word_l:\n",
    "        if word not in model:\n",
    "            return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "train_vec = [model[x].sum(axis = 0) if len(x) and in_vocab(x) else np.zeros((shape_n)) for x in train_data['preprocess_2']]\n",
    "test_vec  = [model[x].sum(axis = 0) if len(x) and in_vocab(x) else np.zeros((shape_n)) for x in test_data['preprocess_2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec198d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.642331635540464\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.56      0.62      1001\n",
      "           1       0.56      0.73      0.64      1430\n",
      "           2       0.74      0.60      0.67      1103\n",
      "\n",
      "    accuracy                           0.64      3534\n",
      "   macro avg       0.67      0.63      0.64      3534\n",
      "weighted avg       0.66      0.64      0.64      3534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "model.fit(train_vec, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "predict = model.predict(test_vec)\n",
    "print(\"Accuracy Score :\", accuracy_score(y_test, predict), end='\\n\\n')\n",
    "print(classification_report(y_true = y_test, y_pred = predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23fc44",
   "metadata": {},
   "source": [
    "### Classification using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f4f7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"What is not to like about this product.\n",
    "Not bad.\n",
    "Not an issue.\n",
    "Not buggy.\n",
    "Not happy.\n",
    "Not user-friendly.\n",
    "Not good.\n",
    "Is it any good?\n",
    "I do not dislike horror movies. \n",
    "Disliking horror movies is not uncommon. \n",
    "Sometimes I really hate the show. \n",
    "I love having to wait two months for the next series to come out! \n",
    "The final episode was surprising with a terrible twist at the end.\n",
    "The film was easy to watch but I would not recommend it to my friends. \n",
    "I LOL’d at the end of the cake scene.\"\"\"\n",
    "\n",
    "input_text = text.split(\"\\n\")\n",
    "input_text = [\" \".join(preprocessing_2(string)) for string in input_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "929d6a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "\n",
    "train_idf = tf_idf.fit_transform(train_data[\"documents\"])\n",
    "pred_idf = tf_idf.transform(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d612c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(max_iter = 1000)\n",
    "model.fit(train_idf, y_train)\n",
    "\n",
    "predict = model.predict(pred_idf)\n",
    "predict = le.inverse_transform(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c09bb42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is not to like about this product.  :  negative\n",
      "Not bad.  :  negative\n",
      "Not an issue.  :  negative\n",
      "Not buggy.  :  neutral\n",
      "Not happy.  :  positive\n",
      "Not user-friendly.  :  neutral\n",
      "Not good.  :  positive\n",
      "Is it any good?  :  positive\n",
      "I do not dislike horror movies.   :  negative\n",
      "Disliking horror movies is not uncommon.   :  negative\n",
      "Sometimes I really hate the show.   :  negative\n",
      "I love having to wait two months for the next series to come out!   :  positive\n",
      "The final episode was surprising with a terrible twist at the end.  :  neutral\n",
      "The film was easy to watch but I would not recommend it to my friends.   :  neutral\n",
      "I LOL’d at the end of the cake scene.  :  neutral\n"
     ]
    }
   ],
   "source": [
    "for index, text in enumerate(text.split(\"\\n\")):\n",
    "    print(text, \" : \", predict[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ce27b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (6.5.4)\n",
      "Requirement already satisfied: lxml in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (4.9.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (4.11.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (0.4)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (3.1.3)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (5.2.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (2.1.4)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (0.8.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (0.5.13)\n",
      "Requirement already satisfied: nbformat>=5.1 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (5.7.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (23.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (2.17.2)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (1.2.1)\n",
      "Requirement already satisfied: traitlets>=5.0 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbconvert) (5.7.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from jupyter-core>=4.7->nbconvert) (2.5.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from jupyter-core>=4.7->nbconvert) (305.1)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbclient>=0.5.0->nbconvert) (7.4.9)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbclient>=0.5.0->nbconvert) (1.5.6)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbformat>=5.1->nbconvert) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from nbformat>=5.1->nbconvert) (4.17.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from beautifulsoup4->nbconvert) (2.3.2.post1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from bleach->nbconvert) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from bleach->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert) (23.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from jupyter-client>=6.1.5->nbclient>=0.5.0->nbconvert) (6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for torch: [Errno 2] No such file or directory: 'c:\\\\users\\\\keerthana\\\\miniconda3\\\\lib\\\\site-packages\\\\torch-2.0.0.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "pip install nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b85cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
