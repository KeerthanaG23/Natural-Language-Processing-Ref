{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLgO51ZMmTGT"
   },
   "source": [
    "## Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vZ3oovMUmStT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==2.8.0\n",
      "  Downloading transformers-2.8.0-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 43.5/43.5 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from transformers==2.8.0) (1.25.2)\n",
      "Collecting tokenizers==0.5.2 (from transformers==2.8.0)\n",
      "  Downloading tokenizers-0.5.2.tar.gz (64 kB)\n",
      "     ---------------------------------------- 0.0/64.6 kB ? eta -:--:--\n",
      "     -------------------------------------- - 61.4/64.6 kB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 64.6/64.6 kB 1.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting boto3 (from transformers==2.8.0)\n",
      "  Downloading boto3-1.34.75-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from transformers==2.8.0) (3.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from transformers==2.8.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from transformers==2.8.0) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from transformers==2.8.0) (2022.9.13)\n",
      "Collecting sentencepiece (from transformers==2.8.0)\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting sacremoses (from transformers==2.8.0)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers==2.8.0) (0.4.6)\n",
      "Collecting botocore<1.35.0,>=1.34.75 (from boto3->transformers==2.8.0)\n",
      "  Downloading botocore-1.34.75-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->transformers==2.8.0)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->transformers==2.8.0)\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from requests->transformers==2.8.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from requests->transformers==2.8.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from requests->transformers==2.8.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from requests->transformers==2.8.0) (2023.7.22)\n",
      "Requirement already satisfied: click in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from sacremoses->transformers==2.8.0) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from sacremoses->transformers==2.8.0) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from botocore<1.35.0,>=1.34.75->boto3->transformers==2.8.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\keerthana\\miniconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.75->boto3->transformers==2.8.0) (1.16.0)\n",
      "Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "   ---------------------------------------- 0.0/563.8 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 71.7/563.8 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 184.3/563.8 kB 2.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 286.7/563.8 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 368.6/563.8 kB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 471.0/563.8 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  563.2/563.8 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 563.8/563.8 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading boto3-1.34.75-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.3 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 10.2/139.3 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 92.2/139.3 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 133.1/139.3 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 139.3/139.3 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 61.4/897.5 kB 1.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 153.6/897.5 kB 1.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 276.5/897.5 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 409.6/897.5 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 563.2/897.5 kB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 655.4/897.5 kB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 727.0/897.5 kB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 829.4/897.5 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  890.9/897.5 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 897.5/897.5 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   - -------------------------------------- 30.7/991.5 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 133.1/991.5 kB 2.0 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 235.5/991.5 kB 2.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 358.4/991.5 kB 2.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 440.3/991.5 kB 2.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 501.8/991.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 563.2/991.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 696.3/991.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 778.2/991.5 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 849.9/991.5 kB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 911.4/991.5 kB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 962.6/991.5 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  983.0/991.5 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 991.5/991.5 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading botocore-1.34.75-py3-none-any.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/12.1 MB 1.9 MB/s eta 0:00:07\n",
      "   ---------------------------------------- 0.1/12.1 MB 1.3 MB/s eta 0:00:10\n",
      "   ---------------------------------------- 0.1/12.1 MB 1.4 MB/s eta 0:00:09\n",
      "    --------------------------------------- 0.2/12.1 MB 1.6 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.3/12.1 MB 1.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.4/12.1 MB 1.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.4/12.1 MB 1.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.5/12.1 MB 1.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.6/12.1 MB 1.6 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.7/12.1 MB 1.7 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.8/12.1 MB 1.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 0.9/12.1 MB 1.7 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.0/12.1 MB 1.8 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.1/12.1 MB 1.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.3/12.1 MB 1.9 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.4/12.1 MB 2.0 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.5/12.1 MB 2.0 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 2.0 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.7/12.1 MB 2.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 1.8/12.1 MB 2.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.0/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 2.1/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.1/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.1/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.3/12.1 MB 2.1 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 2.4/12.1 MB 2.1 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.5/12.1 MB 2.1 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.6/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.7/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.9/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.9/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.9/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.9/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.9/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.9/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.9/12.1 MB 2.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.9/12.1 MB 1.9 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.0/12.1 MB 1.8 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.2/12.1 MB 1.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 3.3/12.1 MB 1.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.3/12.1 MB 1.9 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.5/12.1 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.6/12.1 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.7/12.1 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 3.9/12.1 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.0/12.1 MB 1.9 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.2/12.1 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.3/12.1 MB 1.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 4.4/12.1 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.6/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.7/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------- ------------------------ 4.8/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 4.9/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.0/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.1/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 2.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.3/12.1 MB 1.8 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.4/12.1 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.5/12.1 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.7/12.1 MB 1.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.8/12.1 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 5.9/12.1 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.1/12.1 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.2/12.1 MB 1.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.3/12.1 MB 1.9 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.4/12.1 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 6.6/12.1 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.7/12.1 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.0/12.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 7.2/12.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.3/12.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.4/12.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.5/12.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.7/12.1 MB 2.0 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 7.8/12.1 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 7.9/12.1 MB 2.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.1/12.1 MB 2.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.2/12.1 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.3/12.1 MB 2.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 8.5/12.1 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.6/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 8.7/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.8/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.9/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 9.1/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.2/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.3/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.5/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 9.6/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.7/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.8/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 9.9/12.1 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.1/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.2/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.7/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.8/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.1/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.2/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.2/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.5/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.7/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.9/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.0/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
      "   ---------------------------------------- 0.0/82.2 kB ? eta -:--:--\n",
      "   ---------------------------------------  81.9/82.2 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 82.2/82.2 kB 1.5 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [46 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n",
      "ERROR: Could not find a version that satisfies the requirement torch==1.4.0 (from versions: 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2)\n",
      "ERROR: No matching distribution found for torch==1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==2.8.0\n",
    "!pip install torch==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apYJ3Uu6me0L"
   },
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NM4WowCakwqc"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRvZhj4Nm1wb"
   },
   "outputs": [],
   "source": [
    "# initialize the pretrained model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TBJtnYxjnMp7"
   },
   "outputs": [],
   "source": [
    "# input text\n",
    "text = \"\"\"\n",
    "Back in the 1950s, the fathers of the field, Minsky and McCarthy, described artificial intelligence as any task performed by a machine that would have previously been considered to require human intelligence.\n",
    "\n",
    "That's obviously a fairly broad definition, which is why you will sometimes see arguments over whether something is truly AI or not.\n",
    "\n",
    "Modern definitions of what it means to create intelligence are more specific. Francois Chollet, an AI researcher at Google and creator of the machine-learning software library Keras, has said intelligence is tied to a system's ability to adapt and improvise in a new environment, to generalise its knowledge and apply it to unfamiliar scenarios.\n",
    "\n",
    "\"Intelligence is the efficiency with which you acquire new skills at tasks you didn't previously prepare for,\" he said.\n",
    "\n",
    "\"Intelligence is not skill itself; it's not what you can do; it's how well and how efficiently you can learn new things.\"\n",
    "\n",
    "It's a definition under which modern AI-powered systems, such as virtual assistants, would be characterised as having demonstrated 'narrow AI', the ability to generalise their training when carrying out a limited set of tasks, such as speech recognition or computer vision.\n",
    "\n",
    "Typically, AI systems demonstrate at least some of the following behaviours associated with human intelligence: planning, learning, reasoning, problem-solving, knowledge representation, perception, motion, and manipulation and, to a lesser extent, social intelligence and creativity.\n",
    "\n",
    "AlexNet's performance demonstrated the power of learning systems based on neural networks, a model for machine learning that had existed for decades but that was finally realising its potential due to refinements to architecture and leaps in parallel processing power made possible by Moore's Law. The prowess of machine-learning systems at carrying out computer vision also hit the headlines that year, with Google training a system to recognise an internet favorite: pictures of cats.\n",
    "\n",
    "The next demonstration of the efficacy of machine-learning systems that caught the public's attention was the 2016 triumph of the Google DeepMind AlphaGo AI over a human grandmaster in Go, an ancient Chinese game whose complexity stumped computers for decades. Go has about possible 200 moves per turn compared to about 20 in Chess. Over the course of a game of Go, there are so many possible moves that are searching through each of them in advance to identify the best play is too costly from a computational point of view. Instead, AlphaGo was trained how to play the game by taking moves played by human experts in 30 million Go games and feeding them into deep-learning neural networks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUIY1EQVop_b"
   },
   "outputs": [],
   "source": [
    "## preprocess the input text\n",
    "preprocessed_text = text.strip().replace('\\n','')\n",
    "t5_input_text = 'summarize: ' + preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "cgCbmSLbpXgr",
    "outputId": "efdf5c9a-c973-4858-880f-e9e3c797a9b6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'summarize: Back in the 1950s, the fathers of the field, Minsky and McCarthy, described artificial intelligence as any task performed by a machine that would have previously been considered to require human intelligence.That\\'s obviously a fairly broad definition, which is why you will sometimes see arguments over whether something is truly AI or not.Modern definitions of what it means to create intelligence are more specific. Francois Chollet, an AI researcher at Google and creator of the machine-learning software library Keras, has said intelligence is tied to a system\\'s ability to adapt and improvise in a new environment, to generalise its knowledge and apply it to unfamiliar scenarios.\"Intelligence is the efficiency with which you acquire new skills at tasks you didn\\'t previously prepare for,\" he said.\"Intelligence is not skill itself; it\\'s not what you can do; it\\'s how well and how efficiently you can learn new things.\"It\\'s a definition under which modern AI-powered systems, such as virtual assistants, would be characterised as having demonstrated \\'narrow AI\\', the ability to generalise their training when carrying out a limited set of tasks, such as speech recognition or computer vision.Typically, AI systems demonstrate at least some of the following behaviours associated with human intelligence: planning, learning, reasoning, problem-solving, knowledge representation, perception, motion, and manipulation and, to a lesser extent, social intelligence and creativity.AlexNet\\'s performance demonstrated the power of learning systems based on neural networks, a model for machine learning that had existed for decades but that was finally realising its potential due to refinements to architecture and leaps in parallel processing power made possible by Moore\\'s Law. The prowess of machine-learning systems at carrying out computer vision also hit the headlines that year, with Google training a system to recognise an internet favorite: pictures of cats.The next demonstration of the efficacy of machine-learning systems that caught the public\\'s attention was the 2016 triumph of the Google DeepMind AlphaGo AI over a human grandmaster in Go, an ancient Chinese game whose complexity stumped computers for decades. Go has about possible 200 moves per turn compared to about 20 in Chess. Over the course of a game of Go, there are so many possible moves that are searching through each of them in advance to identify the best play is too costly from a computational point of view. Instead, AlphaGo was trained how to play the game by taking moves played by human experts in 30 million Go games and feeding them into deep-learning neural networks.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogRtl7PZpC3m",
    "outputId": "84e66c5b-f5a9-49b0-8067-f174656977ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "410"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t5_input_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfGLHnLUpXKK"
   },
   "outputs": [],
   "source": [
    "tokenized_text = tokenizer.encode(t5_input_text, return_tensors='pt', max_length=512).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yx_Q72vqGCb"
   },
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQEq-Y0_p5gr"
   },
   "outputs": [],
   "source": [
    "summary_ids = model.generate(tokenized_text, min_length=30, max_length=120)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "nqEQhRI4qcfb",
    "outputId": "f66346db-5559-459f-d220-16f8fefab917"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"artificial intelligence is a task performed by a machine that would have previously been considered to require human intelligence. it's a definition under which modern AI-powered systems, such as virtual assistants, would be characterised as having demonstrated 'narrow AI' the ability to generalise training when carrying out a limited set of tasks, such as speech recognition or computer vision.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ou8vEjdeq-dS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Abstractive Text Summarization - Transformer Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
